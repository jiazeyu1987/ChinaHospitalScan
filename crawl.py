import asyncio
import datetime
import logging
import os
import re
import sqlite3
import sys
import time
from typing import Dict, Set, Any
from urllib.parse import urlparse, urljoin

# é…ç½®çˆ¬è™«ä¸“ç”¨æ—¥å¿—å™¨
crawler_logger = logging.getLogger('crawler')

# å¦‚æœè¿˜æ²¡æœ‰é…ç½®çˆ¬è™«æ—¥å¿—å™¨ï¼Œåˆ™è¿›è¡Œé…ç½®
if not crawler_logger.handlers:
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)

    # æ·»åŠ çˆ¬è™«ä¸“ç”¨æ–‡ä»¶å¤„ç†å™¨
    crawler_file_handler = logging.FileHandler('logs/crawler.log', encoding='utf-8', mode='a')
    crawler_file_handler.setLevel(logging.DEBUG)
    crawler_file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    crawler_file_handler.setFormatter(crawler_file_formatter)

    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    crawler_console_handler = logging.StreamHandler()
    crawler_console_handler.setLevel(logging.INFO)
    crawler_console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    crawler_console_handler.setFormatter(crawler_console_formatter)

    crawler_logger.addHandler(crawler_file_handler)
    crawler_logger.addHandler(crawler_console_handler)
    crawler_logger.setLevel(logging.DEBUG)

    # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°æ ¹æ—¥å¿—å™¨ï¼Œé¿å…é‡å¤
    crawler_logger.propagate = False

import nest_asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import (
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
)
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.deep_crawling.filters import DomainFilter, ContentTypeFilter
from crawl4ai.deep_crawling import FilterChain

# Apply nest_asyncio to handle Windows asyncio limitations
if sys.platform == "win32":
    nest_asyncio.apply()


def clean_text_encoding(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ä¸­çš„å­—ç¬¦ç¼–ç é—®é¢˜ï¼Œç§»é™¤æˆ–æ›¿æ¢æ— æ•ˆå­—ç¬¦
    """
    if not text:
        return text

    # ç§»é™¤æˆ–æ›¿æ¢å¸¸è§çš„ç¼–ç é—®é¢˜å­—ç¬¦
    cleaned = text

    # æ›¿æ¢å¸¸è§çš„æ— æ•ˆå­—ç¬¦
    invalid_chars = ['\uFFFD', '\x00', '\u200B', '\u200C', '\u200D', '\uFEFF']
    for char in invalid_chars:
        cleaned = cleaned.replace(char, '')

    # å¤„ç†è¿ç»­çš„ç©ºç™½å­—ç¬¦
    cleaned = ' '.join(cleaned.split())

    return cleaned.strip()


# é»˜è®¤å…³é”®è¯ï¼Œå¯ä»¥è¢«åŠ¨æ€å…³é”®è¯è¦†ç›–
# æ‰©å±•é»˜è®¤å…³é”®è¯åˆ—è¡¨ï¼Œæé«˜åŒ¹é…ç‡ï¼Œæ·»åŠ åŒ»é™¢ç‰¹å®šè¯æ±‡
DEFAULT_KEYWORDS = (
    # æ ¸å¿ƒé‡‡è´­ç›¸å…³è¯æ±‡
    "å…¬å‘Š", "é‡‡è´­", "å…¬å¼€", "æ‹›æ ‡", "è¯¢ä»·", "æŠ•æ ‡", "ä¸­æ ‡",
    "ä¿¡æ¯", "é€šçŸ¥", "ç»“æœ", "å…¬ç¤º", "æˆäº¤", "æˆäº¤å…¬å‘Š",
    "ç­”ç–‘", "å˜æ›´", "æ¾„æ¸…", "æ›´æ­£", "ä¿®æ”¹", "è¡¥å……",

    # æ‹›æ ‡æµç¨‹ç›¸å…³
    "é¢„å®¡", "è°ˆåˆ¤", "ç«äº‰", "ç£‹å•†", "å•ä¸€æ¥æº", "é‚€è¯·æ‹›æ ‡",
    "å…¬å¼€æ‹›æ ‡", "ç«äº‰æ€§è°ˆåˆ¤", "è¯¢ä»·é‡‡è´­", "ç«äº‰æ€§ç£‹å•†",

    # æ„è§å¾é›†ç›¸å…³
    "å¾æ±‚æ„è§", "å¾æ±‚æ„è§ç¨¿", "å¾é›†", "å…¬ç¤º", "å…¬ç¤ºæœŸ",

    # é¢„ç®—å’Œè®¡åˆ’ç›¸å…³
    "é¢„ç®—", "é‡‡è´­è®¡åˆ’", "éœ€æ±‚", "éœ€æ±‚å…¬ç¤º", "é‡‡è´­éœ€æ±‚",

    # ç»“æœå’ŒåˆåŒç›¸å…³
    "åˆåŒ", "åˆåŒå…¬å‘Š", "å±¥çº¦", "éªŒæ”¶", "è¯„ä¼°",

    # åŒ»é™¢ç‰¹å®šè¯æ±‡
    "åŒ»ç–—", "è¯å“", "å™¨æ¢°", "è®¾å¤‡", "è€—æ", "æœåŠ¡",
    "å«ç”Ÿ", "åŒ»é™¢", "ç–¾æ§", "åŒ»ä¿", "æ–°å†œåˆ",

    # é‡‡è´­æ–¹å¼ç›¸å…³
    "æ‹›æ ‡é‡‡è´­", "è¯¢ä»·é‡‡è´­", "è°ˆåˆ¤é‡‡è´­", "å•ä¸€æ¥æºé‡‡è´­", "ç½‘ä¸Šé‡‡è´­",

    # é‡‘é¢å’Œæ—¶é—´ç›¸å…³
    "ä¸‡å…ƒ", "å…ƒ", "æŠ¥ä»·", "é¢„ç®—é‡‘é¢", "ä¸­æ ‡é‡‘é¢",
    "æˆªæ­¢æ—¶é—´", "å¼€æ ‡æ—¶é—´", "æŠ•æ ‡æ—¶é—´", "å…¬ç¤ºæ—¶é—´",

    # å…¶ä»–ç›¸å…³è¯æ±‡
    "ä¾›åº”å•†", "æŠ•æ ‡äºº", "ä¸­æ ‡äºº", "æˆäº¤äºº", "é¡¹ç›®", "é¡¹ç›®ç¼–å·"
)


def _has_keyword(text: str | None, keywords: tuple = None) -> bool:
    """
    åˆ¤æ–­é“¾æ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä»»æ„ä¸€ä¸ªç›®æ ‡å…³é”®è¯ã€‚

    Args:
        text: è¦æ£€æŸ¥çš„æ–‡æœ¬
        keywords: å…³é”®è¯å…ƒç»„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å…³é”®è¯

    Returns:
        bool: æ˜¯å¦åŒ…å«å…³é”®è¯
    """
    if not text:
        logging.debug(f"ğŸ” [KEYWORD_FILTER] æ£€æŸ¥å…³é”®è¯: æ–‡æœ¬ä¸ºç©º")
        return False

    # ä½¿ç”¨ä¼ å…¥çš„å…³é”®è¯æˆ–é»˜è®¤å…³é”®è¯
    target_keywords = keywords or DEFAULT_KEYWORDS
    text_lower = text.lower()

    logging.info(f"ğŸ” [KEYWORD_FILTER] å¼€å§‹å…³é”®è¯åŒ¹é…æ£€æŸ¥")
    logging.info(f"   åŸå§‹æ–‡æœ¬: '{text}'")
    logging.info(f"   æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    logging.info(f"   å…³é”®è¯æ•°é‡: {len(target_keywords)} ä¸ª")

    matched_keywords = []
    for kw in target_keywords:
        if kw and kw.lower() in text_lower:
            matched_keywords.append(kw)
            logging.info(f"âœ… [KEYWORD_FILTER] åŒ¹é…æˆåŠŸ: å…³é”®è¯ '{kw}' åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°")
        else:
            logging.debug(f"   - æœªåŒ¹é…å…³é”®è¯: '{kw}'")

    if matched_keywords:
        logging.info(f"ğŸ¯ [KEYWORD_FILTER] æœ€ç»ˆåŒ¹é…ç»“æœ: æˆåŠŸ")
        logging.info(f"   åŒ¹é…çš„å…³é”®è¯: {matched_keywords}")
        return True
    else:
        logging.info(f"âŒ [KEYWORD_FILTER] æœ€ç»ˆåŒ¹é…ç»“æœ: å¤±è´¥")
        logging.info(f"   æ‰€æœ‰å…³é”®è¯å‡æœªåŒ¹é…")
        return False


def _is_html_page(url: str, unlimited_mode: bool = False) -> bool:
    """
    åˆ¤æ–­URLæ˜¯å¦ä¸ºHTMLé¡µé¢ã€‚
    æ”¯æŒå¤šç§URLæ ¼å¼ï¼š.html/.htm/.shtmlåç¼€ã€æ— åç¼€è·¯å¾„ã€åŠ¨æ€å‚æ•°ç­‰ã€‚
    è¿‡æ»¤æ‰æ˜æ˜¾çš„é™æ€èµ„æºï¼ˆå›¾ç‰‡ã€CSSã€JS ç­‰ï¼‰ã€‚

    åœ¨æ— é™åˆ¶æ¨¡å¼ä¸‹ï¼Œæ”¾è¡Œæ‰€æœ‰åŒåŸŸURLã€‚
    """
    try:
        logging.info(f"ğŸ” [INFO] _is_html_page called with url={url}, unlimited_mode={unlimited_mode}")

        # å¼ºåˆ¶æ£€æµ‹æ— é™åˆ¶æ¨¡å¼ï¼šå¦‚æœæ˜¯é•¿æµ·åŒ»é™¢åŸŸåï¼Œè‡ªåŠ¨åº”ç”¨æ— é™åˆ¶æ¨¡å¼
        if "longhua.net" in url:
            forced_unlimited = True
            logging.info(f"ğŸ”¥ [FORCE_UNLIMITED] æ£€æµ‹åˆ°é•¿æµ·åŒ»é™¢åŸŸåï¼Œå¼ºåˆ¶åº”ç”¨æ— é™åˆ¶æ¨¡å¼: {url}")
            return True

        # åŸæœ‰çš„æ— é™åˆ¶æ¨¡å¼ï¼šæ”¾è¡Œæ‰€æœ‰URL
        if unlimited_mode:
            logging.info(f"ğŸ”¥ [UNLIMITED_MODE] URLè¿‡æ»¤æ”¾å¼€: {url}")
            return True

        parsed = urlparse(url)
        path = parsed.path or ""

        # è¯¦ç»†æ—¥å¿—è®°å½•
        logging.debug(f"ğŸ” [URL_FILTER] æ£€æŸ¥URL: {url}")
        logging.debug(f"   è·¯å¾„: '{path}', æŸ¥è¯¢å‚æ•°: '{parsed.query}'")

        # 1. æ˜ç¡®çš„HTMLåç¼€
        if path.lower().endswith((".html", ".htm", ".shtml")):
            logging.debug(f"   âœ… é€šè¿‡HTMLåç¼€æ£€æŸ¥")
            return True

        # 2. æ²¡æœ‰åç¼€çš„è·¯å¾„ï¼ˆå¯èƒ½æ˜¯åŠ¨æ€é¡µé¢ï¼‰
        if not path or path == "/" or not any(path.lower().endswith(ext) for ext in
            ['.css', '.js', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.ico', '.pdf',
             '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.tar', '.gz',
             '.mp3', '.mp4', '.avi', '.mov', '.flv', '.wmv']):
            logging.debug(f"   âœ… é€šè¿‡æ— åç¼€è·¯å¾„æ£€æŸ¥ï¼ˆå¯èƒ½ä¸ºåŠ¨æ€é¡µé¢ï¼‰")
            return True

        # 3. å¸¸è§çš„CMSæˆ–ç³»ç»Ÿè·¯å¾„æ¨¡å¼
        cms_patterns = [
            '/index', '/list', '/detail', '/view', '/show', '/article', '/news',
            '/notice', '/info', '/content', '/page', '/item', '/cggg', '/tender'
        ]
        if any(pattern in path.lower() for pattern in cms_patterns):
            logging.debug(f"   âœ… é€šè¿‡CMSè·¯å¾„æ¨¡å¼æ£€æŸ¥")
            return True

        # 4. åŒ…å«æŸ¥è¯¢å‚æ•°çš„URLï¼ˆé€šå¸¸æ˜¯åŠ¨æ€é¡µé¢ï¼‰
        if parsed.query:
            # æ£€æŸ¥æ˜¯å¦ä¸æ˜¯æ˜æ˜¾çš„èµ„æºæ–‡ä»¶
            if not any(path.lower().endswith(ext) for ext in
                ['.css', '.js', '.jpg', '.jpeg', '.png', '.gif', '.svg']):
                logging.debug(f"   âœ… é€šè¿‡æŸ¥è¯¢å‚æ•°æ£€æŸ¥ï¼ˆåŠ¨æ€é¡µé¢ï¼‰")
                return True

        logging.debug(f"   âŒ è¿‡æ»¤åŸå› : ä¸ç¬¦åˆHTMLé¡µé¢ç‰¹å¾")
        return False

    except Exception as e:
        logging.error(f"âŒ [URL_FILTER] è§£æURLå¤±è´¥: {url}, é”™è¯¯: {e}")
        return False


def init_db(db_path: str) -> sqlite3.Connection:
    """Initialize SQLite database and links table."""
    try:
        logging.info(f"ğŸ—„ï¸ [DATABASE] åˆå§‹åŒ–æ•°æ®åº“: {db_path}")

        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        db_dir = os.path.dirname(db_path)
        if db_dir:
            os.makedirs(db_dir, exist_ok=True)
            logging.debug(f"ğŸ“ [DATABASE] ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨: {db_dir}")

        # è¿æ¥æ•°æ®åº“ï¼Œå¢åŠ è¶…æ—¶å’ŒWALæ¨¡å¼
        logging.debug(f"ğŸ”Œ [DATABASE] è¿æ¥æ•°æ®åº“ï¼Œè¶…æ—¶30ç§’")
        conn = sqlite3.connect(db_path, timeout=30.0)

        try:
            conn.execute("PRAGMA journal_mode=WAL")
            logging.debug("âœ… [DATABASE] WALæ¨¡å¼å¯ç”¨æˆåŠŸ")
        except sqlite3.Error as e:
            logging.warning(f"âš ï¸ [DATABASE] WALæ¨¡å¼å¯ç”¨å¤±è´¥: {e}ï¼Œç»§ç»­ä½¿ç”¨é»˜è®¤æ¨¡å¼")

        cursor = conn.cursor()

        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='procurement_links'"
        )
        table_exists = cursor.fetchone() is not None

        if table_exists:
            logging.info("ğŸ“‹ [DATABASE] procurement_linksè¡¨å·²å­˜åœ¨")
            # æ£€æŸ¥è¡¨ç»“æ„
            cursor.execute("PRAGMA table_info(procurement_links)")
            columns = [row[1] for row in cursor.fetchall()]
            logging.debug(f"ğŸ“‹ [DATABASE] ç°æœ‰åˆ—: {columns}")
        else:
            logging.info("ğŸ—ï¸ [DATABASE] åˆ›å»ºprocurement_linksè¡¨")

        # Create table if not exists (full schema for new DBs)
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS procurement_links (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                base_url TEXT NOT NULL,
                url TEXT NOT NULL,
                link_text TEXT,
                first_seen_at TEXT,
                last_seen_at TEXT,
                is_latest INTEGER DEFAULT 0,
                UNIQUE(base_url, url)
            )
            """
        )

        if not table_exists:
            logging.info("âœ… [DATABASE] procurement_linksè¡¨åˆ›å»ºæˆåŠŸ")

        # éªŒè¯è¡¨æ˜¯å¦å¯è®¿é—®
        cursor.execute("SELECT COUNT(*) FROM procurement_links")
        count = cursor.fetchone()[0]
        logging.info(f"ğŸ“Š [DATABASE] procurement_linksè¡¨å½“å‰è®°å½•æ•°: {count}")

        conn.commit()
        logging.info("âœ… [DATABASE] æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        return conn

    except sqlite3.Error as e:
        logging.error(f"âŒ [DATABASE] æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
    except Exception as e:
        logging.error(f"âŒ [DATABASE] åˆå§‹åŒ–è¿‡ç¨‹å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        raise

  

async def fallback_crawl_procurement_links(
    base_url: str,
    max_depth: int | None = None,
    max_pages: int | None = None,
    keywords: list[str] | None = None,
) -> Dict[str, Any]:
    """
    Fallback crawling method using requests library when Playwright fails.
    ä½¿ç”¨ requests + BeautifulSoup å®ç°ä¸€ä¸ªç®€å•çš„ BFS çˆ¬å–ï¼Œ
    max_depth / max_pages å‚æ•°ä¸ BFSDeepCrawlStrategy å«ä¹‰ä¸€è‡´ã€‚
    """
    import requests
    from bs4 import BeautifulSoup

    start_time = time.time()

    logging.info(f"ğŸš€ [FALLBACK_CRAWLER] å¼€å§‹Fallbackçˆ¬å–ä»»åŠ¡")
    logging.info(f"ğŸ“‹ [FALLBACK_CRAWLER] åŸºç¡€URL: {base_url}")
    logging.info(f"âš™ï¸ [FALLBACK_CRAWLER] å‚æ•°é…ç½® - max_depth: {max_depth}, max_pages: {max_pages}")
    logging.info(f"ğŸ”‘ [FALLBACK_CRAWLER] å…³é”®è¯: {keywords if keywords else 'ä½¿ç”¨é»˜è®¤å…³é”®è¯'}")

    if not base_url:
        logging.error(f"âŒ [FALLBACK_CRAWLER] base_urlä¸èƒ½ä¸ºç©º")
        raise ValueError("base_url must not be empty")

    # æ£€æµ‹æ˜¯å¦ä¸ºæ— é™åˆ¶æ¨¡å¼
    unlimited_mode = (max_depth and max_depth >= 20) and (max_pages and max_pages >= 500)
    if unlimited_mode:
        logging.info(f"ğŸ”¥ [UNLIMITED_MODE] æ£€æµ‹åˆ°æ— é™åˆ¶æ¨¡å¼ - å®Œå…¨æ”¾å¼€æ‰€æœ‰é™åˆ¶!")
        logging.info(f"   - max_depth: {max_depth} (>=20)")
        logging.info(f"   - max_pages: {max_pages} (>=500)")
        logging.info(f"   - URLè¿‡æ»¤: æ”¾å¼€")
        logging.info(f"   - å…³é”®è¯è¿‡æ»¤: æ”¾å¼€")

    # Extract domain for filtering
    domain_match = re.search(r"https?://([^/]+)", base_url)
    domain = domain_match.group(1) if domain_match else "hospital-cqmu.com"
    logging.info(f"ğŸŒ [FALLBACK_CRAWLER] è§£æåŸŸå: {domain}")

    # SQLite database path (ä½¿ç”¨ä¸ä¸»åº”ç”¨ç›¸åŒçš„æ•°æ®åº“è·¯å¾„)
    db_path = os.path.abspath(os.path.join("data", "hospital_scanner_new.db"))
    logging.info(f"ğŸ—„ï¸ [FALLBACK_CRAWLER] æ•°æ®åº“è·¯å¾„: {db_path}")
    logging.debug(f"ğŸ“ [FALLBACK_CRAWLER] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

    conn = init_db(db_path)
    cursor = conn.cursor()

    # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨å¹¶ç»Ÿè®¡ç°æœ‰è®°å½•
    cursor.execute("SELECT COUNT(*) FROM procurement_links")
    before_count = cursor.fetchone()[0]
    logging.info(f"ğŸ“Š [FALLBACK_CRAWLER] çˆ¬è™«å‰è®°å½•æ•°: {before_count}")

    # Current run timestamp
    now = datetime.datetime.utcnow().isoformat(timespec="seconds")

    # Before this run, mark previous "latest" records for this base_url as not latest
    logging.info(f"ğŸ”„ [FALLBACK_CRAWLER] æ ‡è®°ä¹‹å‰çš„latestè®°å½•ä¸ºéæœ€æ–°çŠ¶æ€")
    try:
        cursor.execute(
            "UPDATE procurement_links SET is_latest = 0 WHERE base_url = ?",
            (base_url,),
        )
        updated_count = cursor.rowcount
        logging.info(f"âœ… [FALLBACK_CRAWLER] å·²æ ‡è®° {updated_count} æ¡æ—§è®°å½•ä¸ºéæœ€æ–°çŠ¶æ€")
    except sqlite3.Error as e:
        logging.error(f"âŒ [FALLBACK_CRAWLER] æ›´æ–°latestçŠ¶æ€å¤±è´¥: {e}")
        raise

    # Store unique URLs and their link textï¼ˆä»…è®°å½• html / htm åç¼€çš„é¡µé¢ï¼‰
    all_raw_urls: Set[str] = set()
    url_to_text: Dict[str, str] = {}

    # BFS é˜Ÿåˆ—ï¼Œå…ƒç´ ä¸º (url, depth)ï¼Œèµ·ç‚¹å¯ä»¥æ˜¯æ— åç¼€åˆ—è¡¨é¡µï¼Œä½†åªè®°å½• HTML è¯¦æƒ…é¡µ
    # å¢åŠ é»˜è®¤å‚æ•°ä»¥æé«˜è¦†ç›–ç‡
    max_depth_val = max_depth or 10  # ä»5å¢åŠ åˆ°10
    max_pages_val = max_pages or 100  # ä»27å¢åŠ åˆ°100
    queue: list[tuple[str, int]] = [(base_url, 0)]
    visited_pages: Set[str] = set()

    # æ— é™åˆ¶æ¨¡å¼å·²åœ¨å‡½æ•°å¼€å§‹æ—¶æ£€æµ‹ (unlimited_modeå˜é‡)

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    logging.info(f"ğŸ” [FALLBACK_CRAWLER] å¼€å§‹BFSçˆ¬å–: max_depth={max_depth_val}, max_pages={max_pages_val}")
    logging.info(f"ğŸ“‹ [FALLBACK_CRAWLER] åˆå§‹é˜Ÿåˆ—é•¿åº¦: {len(queue)}")

    processed_count = 0
    try:
        while queue and len(visited_pages) < max_pages_val:
            current_url, depth = queue.pop(0)
            processed_count += 1

            logging.debug(f"ğŸ”„ [FALLBACK_CRAWLER] [{processed_count}/{len(visited_pages)+1}] å¤„ç†URL: {current_url}, depth: {depth}")

            if current_url in visited_pages:
                logging.debug(f"â­ï¸ [FALLBACK_CRAWLER] URLå·²è®¿é—®ï¼Œè·³è¿‡: {current_url}")
                continue

            if depth > max_depth_val:
                logging.debug(f"â­ï¸ [FALLBACK_CRAWLER] è¶…å‡ºæœ€å¤§æ·±åº¦ï¼Œè·³è¿‡: {current_url}, depth: {depth}")
                continue

            visited_pages.add(current_url)
            logging.info(f"ğŸŒ [FALLBACK_CRAWLER] å¼€å§‹è¯·æ±‚é¡µé¢: {current_url}")

            try:
                response = requests.get(current_url, headers=headers, timeout=30)
                response.raise_for_status()
                logging.info(f"âœ… [FALLBACK_CRAWLER] é¡µé¢è¯·æ±‚æˆåŠŸ: {current_url}")
                logging.debug(f"   çŠ¶æ€ç : {response.status_code}")
                logging.debug(f"   å†…å®¹å¤§å°: {len(response.content)} bytes")
                logging.debug(f"   Content-Type: {response.headers.get('content-type', 'unknown')}")
            except Exception as e:
                logging.error(f"âŒ [FALLBACK_CRAWLER] é¡µé¢è¯·æ±‚å¤±è´¥: {current_url}")
                logging.error(f"   é”™è¯¯è¯¦æƒ…: {e}")
                continue

            # ä»…è®°å½• html / htm é¡µé¢
            if _is_html_page(current_url) and current_url not in all_raw_urls:
                all_raw_urls.add(current_url)
                logging.info(f"ğŸ“„ [FALLBACK_CRAWLER] å‘ç°HTMLé¡µé¢: {current_url}")

            soup = BeautifulSoup(response.content, "html.parser")
            found_links_on_page = 0
            processed_links_on_page = 0

            logging.debug(f"ğŸ” [FALLBACK_CRAWLER] å¼€å§‹è§£æé¡µé¢é“¾æ¥: {current_url}")

            # Extract all links on this page
            for link in soup.find_all("a", href=True):
                href = link.get("href")
                text = link.get_text(strip=True)
                processed_links_on_page += 1

                if not href:
                    logging.debug(f"â­ï¸ [FALLBACK_CRAWLER] è·³è¿‡ç©ºé“¾æ¥")
                    continue

                # è¯¦ç»†æ—¥å¿—ï¼šè®°å½•å‰10ä¸ªå‘ç°çš„é“¾æ¥
                if processed_links_on_page <= 10:
                    logging.info(f"ğŸ”— [FALLBACK_CRAWLER] å‘ç°é“¾æ¥ #{processed_links_on_page}: href='{href}', text='{text[:50]}...'")

                # Convert relative URLs to absolute
                original_href = href
                if href.startswith("/"):
                    href = f"https://{domain}{href}"
                    logging.debug(f"ğŸ”§ [FALLBACK_CRAWLER] è½¬æ¢ç›¸å¯¹URL: {original_href} -> {href}")
                elif href.startswith("#"):
                    if processed_links_on_page <= 5:  # åªè®°å½•å‰5ä¸ªé”šç‚¹é“¾æ¥
                        logging.info(f"â­ï¸ [FALLBACK_CRAWLER] è·³è¿‡é”šç‚¹é“¾æ¥: {href}")
                    continue  # Skip anchors
                elif not href.startswith("http"):
                    # ğŸ”§ [FIX] ä½¿ç”¨urljoinè½¬æ¢ç›¸å¯¹é“¾æ¥ä¸ºç»å¯¹é“¾æ¥
                    new_href = urljoin(current_url, href)
                    logging.info(f"ğŸ”§ [CONVERT] BeautifulSoupç›¸å¯¹é“¾æ¥è½¬æ¢: {href} -> {new_href}")
                    href = new_href
                    # ä¸å†continueï¼Œç»§ç»­åç»­å¤„ç†

                # åªä¿ç•™åŒåŸŸé“¾æ¥å‚ä¸åç»­éå†
                if domain not in href:
                    if processed_links_on_page <= 5:  # åªè®°å½•å‰5ä¸ªå¤–åŸŸé“¾æ¥
                        logging.info(f"ğŸš« [FALLBACK_CRAWLER] è·³è¿‡å¤–åŸŸé“¾æ¥: {href}")
                    continue

                # HTMLé¡µé¢æ£€æŸ¥
                html_check_result = _is_html_page(href, unlimited_mode)
                if html_check_result:
                    all_raw_urls.add(href)
                    if text:
                        url_to_text[href] = text
                    else:
                        # æ— é™åˆ¶æ¨¡å¼ï¼šè®°å½•æ‰€æœ‰æœ‰æ„ä¹‰çš„é“¾æ¥
                        logging.info(f"ğŸ”¥ [UNLIMITED_MODE] å¤„ç†é“¾æ¥: {href}")
                        logging.info(f"   æ–‡æœ¬: '{text}'")
                        logging.info(f"   å°†è¢«è®°å½•åˆ°æ•°æ®åº“: æ˜¯")

                found_links_on_page += 1

                # æ— è®ºæ˜¯å¦ä¸º htmlï¼Œåªè¦åŒåŸŸä¸”æ»¡è¶³æ·±åº¦/æ•°é‡é™åˆ¶ï¼Œéƒ½å¯ä»¥è¿›å…¥ BFS é˜Ÿåˆ—
                if (
                    href not in visited_pages
                    and depth + 1 <= max_depth_val
                    and len(visited_pages) + len(queue) < max_pages_val
                ):
                    queue.append((href, depth + 1))
                    logging.debug(f"â• [FALLBACK_CRAWLER] åŠ å…¥é˜Ÿåˆ—: {href} (depth {depth + 1})")

            logging.info(f"ğŸ“Š [FALLBACK_CRAWLER] é¡µé¢é“¾æ¥è§£æå®Œæˆ: {current_url}")
            logging.info(f"   å¤„ç†é“¾æ¥æ•°: {processed_links_on_page}, å‘ç°æœ‰æ•ˆé“¾æ¥æ•°: {found_links_on_page}")
            logging.info(f"   é˜Ÿåˆ—å½“å‰é•¿åº¦: {len(queue)}")
            logging.info(f"   å·²è®¿é—®é¡µé¢æ•°: {len(visited_pages)}")
            logging.info(f"   å‘ç°HTMLé¡µé¢æ€»æ•°: {len(all_raw_urls)}")

    except Exception as e:
        print(f"Fallback crawling failed: {e}")
        raise

    # Write all unique URLs into database
    new_or_updated = 0
    for raw_url in sorted(all_raw_urls):
        link_text = url_to_text.get(raw_url)

        # æ— é™åˆ¶æ¨¡å¼ï¼šè·³è¿‡å…³é”®è¯è¿‡æ»¤ï¼Œç›´æ¥å†™å…¥æ•°æ®åº“
        if unlimited_mode:
            logging.info(f"ğŸ”¥ [UNLIMITED_MODE] å…³é”®è¯è¿‡æ»¤æ”¾å¼€: {raw_url}")
            if link_text:
                logging.info(f"   é“¾æ¥æ–‡æœ¬: '{link_text[:100]}...'")
        else:
            # Apply dynamic keyword filter if provided; otherwise fall back to built-in keywords
            if keywords:
                text_for_match = link_text or ""
                if not any(kw and kw in text_for_match for kw in keywords):
                    continue
            else:
                if not _has_keyword(link_text, tuple(keywords) if keywords else None, unlimited_mode):
                    continue
        try:
            # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
            cursor.execute(
                "SELECT COUNT(*) FROM procurement_links WHERE base_url = ? AND url = ?",
                (base_url, raw_url)
            )
            exists = cursor.fetchone()[0] > 0

            if exists:
                # è®°å½•å·²å­˜åœ¨ï¼Œæ‰§è¡ŒUPDATE
                cursor.execute(
                    """
                    UPDATE procurement_links SET
                        link_text = COALESCE(?, procurement_links.link_text),
                        last_seen_at = ?,
                        is_latest = 1
                    WHERE base_url = ? AND url = ?
                    """,
                    (link_text, now, base_url, raw_url),
                )
                print(f"ğŸ”„ æ›´æ–°è®°å½•: {raw_url[:80]}")
            else:
                # è®°å½•ä¸å­˜åœ¨ï¼Œæ‰§è¡ŒINSERT
                cursor.execute(
                    """
                    INSERT INTO procurement_links (
                        base_url,
                        url,
                        link_text,
                        first_seen_at,
                        last_seen_at,
                        is_latest
                    )
                    VALUES (?, ?, ?, ?, ?, 1)
                    """,
                    (base_url, raw_url, link_text, now, now),
                )
                print(f"â• æ–°å¢è®°å½•: {raw_url[:80]}")

            # æ— è®ºæ˜¯INSERTè¿˜æ˜¯UPDATEéƒ½ç®—ä½œæ›´æ–°
            new_or_updated += 1

        except sqlite3.Error as e:
            print(f"DB write failed for {raw_url}: {e}")
            import traceback
            traceback.print_exc()

    conn.commit()

    # éªŒè¯å†™å…¥ç»“æœ
    cursor.execute("SELECT COUNT(*) FROM procurement_links")
    after_count = cursor.fetchone()[0]
    print(f"ğŸ“Š çˆ¬è™«åè®°å½•æ•°: {after_count}")
    print(f"ğŸ“Š æ–°å¢è®°å½•æ•°: {after_count - before_count}")

    conn.close()

    print(f"\nFallback crawl finished. URLs written to DB: {db_path}")
    print(f"Database absolute path: {os.path.abspath(db_path)}")
    print(f"Summary: collected {len(all_raw_urls)} unique URLs, inserted/updated {new_or_updated} records")

    return {
        "base_url": base_url,
        "total_urls": len(all_raw_urls),
        "new_or_updated": new_or_updated,
        "db_path": db_path,
    }


async def _crawl_procurement_links_impl(
    base_url: str,
    max_depth: int | None = None,
    max_pages: int | None = None,
    keywords: list[str] | None = None,
) -> Dict[str, Any]:
    """
    Core async implementation to crawl procurement links starting from the given base_url
    and store results into the SQLite database, following the same logic as the original script.
    This function assumes it is running in an event loop that supports asyncio subprocess APIs.
    """
    start_time = time.time()

    logging.info(f"ğŸš€ [CRAWLER] å¼€å§‹é‡‡è´­é“¾æ¥çˆ¬å–ä»»åŠ¡")
    logging.info(f"ğŸ“‹ [CRAWLER] åŸºç¡€URL: {base_url}")
    logging.info(f"âš™ï¸ [CRAWLER] å‚æ•°é…ç½® - max_depth: {max_depth}, max_pages: {max_pages}")
    logging.info(f"ğŸ”‘ [CRAWLER] å…³é”®è¯: {keywords if keywords else 'ä½¿ç”¨é»˜è®¤å…³é”®è¯'}")

    if not base_url:
        logging.error(f"âŒ [CRAWLER] base_urlä¸èƒ½ä¸ºç©º")
        raise ValueError("base_url must not be empty")

    # Extract domain for filtering
    domain_match = re.search(r"https?://([^/]+)", base_url)
    domain = domain_match.group(1) if domain_match else "hospital-cqmu.com"
    logging.info(f"ğŸŒ [CRAWLER] è§£æåŸŸå: {domain}")

    # SQLite database path (ä½¿ç”¨ä¸ä¸»åº”ç”¨ç›¸åŒçš„æ•°æ®åº“è·¯å¾„)
    db_path = os.path.abspath(os.path.join("data", "hospital_scanner_new.db"))
    logging.info(f"ğŸ—„ï¸ [CRAWLER] æ•°æ®åº“è·¯å¾„: {db_path}")

    conn = init_db(db_path)
    cursor = conn.cursor()

    # Current run timestamp (ISO string)
    now = datetime.datetime.utcnow().isoformat(timespec="seconds")

    # Before this run, mark previous "latest" records for this base_url as not latest
    logging.info(f"ğŸ”„ [CRAWLER] æ ‡è®°ä¹‹å‰çš„latestè®°å½•ä¸ºéæœ€æ–°çŠ¶æ€")
    try:
        cursor.execute(
            "UPDATE procurement_links SET is_latest = 0 WHERE base_url = ?",
            (base_url,),
        )
        updated_count = cursor.rowcount
        logging.info(f"âœ… [CRAWLER] å·²æ ‡è®° {updated_count} æ¡æ—§è®°å½•ä¸ºéæœ€æ–°çŠ¶æ€")
    except sqlite3.Error as e:
        logging.error(f"âŒ [CRAWLER] æ›´æ–°latestçŠ¶æ€å¤±è´¥: {e}")
        raise

    # Store unique URLs and their link textï¼ˆä»…è®°å½• html / htm åç¼€çš„é¡µé¢ï¼‰
    all_raw_urls: Set[str] = set()
    url_to_text: Dict[str, str] = {}

    browser_config = BrowserConfig(
        headless=True,
        verbose=False,
        # Windows-specific configuration to handle subprocess issues
        browser_type="chromium" if sys.platform != "win32" else "chromium",
        extra_args=[
            "--no-sandbox",
            "--disable-dev-shm-usage",
            "--disable-gpu",
            "--disable-extensions",
            "--disable-background-timer-throttling",
            "--disable-backgrounding-occluded-windows",
            "--disable-renderer-backgrounding",
            # Character encoding improvements
            "--disable-blink-features=AutomationControlled",
            "--enable-features=NetworkService",
            "--disable-features=VizDisplayCompositor",
            "--disable-web-security",
            "--allow-running-insecure-content"
        ] if sys.platform == "win32" else [
            # Character encoding improvements for non-Windows platforms
            "--enable-features=NetworkService",
            "--disable-blink-features=AutomationControlled"
        ]
    )
    print("max depth:", max_depth, "max pages:", max_pages)
    deep_crawl_strategy = BFSDeepCrawlStrategy(
        max_depth=max_depth or 5,
        max_pages=max_pages or 27,
        include_external=False,
        filter_chain=FilterChain(
            [
                DomainFilter(allowed_domains=[domain]),
                ContentTypeFilter(allowed_types=["text/html"]),
            ]
        ),
    )

    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        deep_crawl_strategy=deep_crawl_strategy,
        stream=True,
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        last_request_time = 0.0

        print(f"Start crawling procurement page: {base_url}")

        async for result in await crawler.arun(
            url=base_url,
            config=run_config,
        ):
            # Throttle requests
            current_time = time.time()
            if current_time - last_request_time < 1.0:
                await asyncio.sleep(1.0 - (current_time - last_request_time))
            last_request_time = current_time
            print(result.url)
            if result.success:
                # 1. Page URL itselfï¼ˆä»…è®°å½• html / htm é¡µé¢ï¼‰
                if _is_html_page(result.url) and result.url not in all_raw_urls:
                    all_raw_urls.add(result.url)
                    print(f"New HTML URL: {result.url}")

                # 2. Links from result.linksï¼ˆä»…è®°å½• html / htm é¡µé¢ï¼‰
                if hasattr(result, "links") and result.links:
                    for link in result.links:
                        if isinstance(link, str):
                            link_url = link
                            link_text = None
                        else:
                            # Support Link(url=..., text=...)
                            link_url = getattr(link, "url", None)
                            link_text = getattr(link, "text", None)

                        if not link_url or domain not in link_url:
                            continue

                        # åªè®°å½• html / htm é¡µé¢
                        if not _is_html_page(link_url):
                            continue

                        if link_url not in all_raw_urls:
                            all_raw_urls.add(link_url)
                            print(f"New HTML URL: {link_url}")

                        if link_text:
                            clean_link_text = clean_text_encoding(link_text.strip())
                            if clean_link_text:
                                url_to_text.setdefault(link_url, clean_link_text)

                # 3. Extract [text](url) from markdownï¼ˆä»…è®°å½• html / htm é¡µé¢ï¼‰
                markdown = result.markdown or ""
                pair_pattern = re.compile(
                    r"\[([^\]]+)\]\((https?://" + re.escape(domain) + r"[^\)]*)\)"
                )
                for link_text, link_url in pair_pattern.findall(markdown):
                    # åªè®°å½• html / htm é¡µé¢
                    if not _is_html_page(link_url):
                        continue

                    if link_url not in all_raw_urls:
                        all_raw_urls.add(link_url)
                        print(f"New HTML URL (markdown): {link_url}")

                    clean_link_text = clean_text_encoding(link_text.strip())
                    if clean_link_text:
                        url_to_text.setdefault(link_url, clean_link_text)

            else:
                print(
                    f"Crawl failed: {getattr(result, 'url', '')} -> {result.error_message}"
                )

    # Write all unique URLs into database
    new_or_updated = 0
    filtered_out = 0

    print(f"ğŸ” [DEBUG] å¼€å§‹å¤„ç† {len(all_raw_urls)} ä¸ªå‘ç°çš„URL...")
    logging.info(f"ğŸ” [CRAWLER] å¼€å§‹å…³é”®è¯è¿‡æ»¤ï¼Œæ€»å…±å‘ç° {len(all_raw_urls)} ä¸ªURL")
    logging.info(f"ğŸ”‘ [CRAWLER] ä½¿ç”¨çš„å…³é”®è¯: {keywords if keywords else 'é»˜è®¤å…³é”®è¯'}")

    processed_count = 0
    for raw_url in sorted(all_raw_urls):
        processed_count += 1
        link_text = url_to_text.get(raw_url)

        logging.debug(f"ğŸ”— [CRAWLER] [{processed_count}/{len(all_raw_urls)}] å¤„ç†é“¾æ¥: {raw_url}")
        logging.debug(f"ğŸ“ [CRAWLER] é“¾æ¥æ–‡æœ¬: '{link_text}'")

        # Apply dynamic keyword filter if provided; otherwise fall back to built-in keywords
        keyword_filter_pass = False

        if keywords:
            text_for_match = link_text or ""
            matched_keywords = [kw for kw in keywords if kw and kw in text_for_match]
            if not matched_keywords:
                logging.debug(f"âŒ [CRAWLER] å…³é”®è¯è¿‡æ»¤å¤±è´¥ - æ–‡æœ¬ä¸­æœªæ‰¾åˆ°å…³é”®è¯: {keywords}")
                logging.debug(f"   å¤±è´¥URL: {raw_url}")
                filtered_out += 1
                keyword_filter_pass = False
            else:
                logging.info(f"âœ… [CRAWLER] å…³é”®è¯åŒ¹é…æˆåŠŸ: {matched_keywords}")
                logging.info(f"   åŒ¹é…URL: {raw_url}")
                logging.info(f"   é“¾æ¥æ–‡æœ¬: '{link_text}'")
                keyword_filter_pass = True
        else:
            # æ— é™åˆ¶æ¨¡å¼æ£€æµ‹
            if unlimited_mode and link_text and len(link_text.strip()) > 3:
                # åœ¨æ— é™åˆ¶æ¨¡å¼ä¸‹ï¼Œå‡ ä¹æ‰€æœ‰æœ‰æ„ä¹‰çš„å†…å®¹éƒ½é€šè¿‡
                text_lower = link_text.lower().strip()

                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text_lower)

                # æ’é™¤çº¯é€šç”¨è¯
                common_words = {"çš„", "å’Œ", "ä¸", "ä¸º", "å¯¹", "åœ¨", "æ˜¯", "æœ‰", "ä¸ª", "ä¸­", "äºº", "å…¬å¸", "åŒ»é™¢", "æ—¶é—´", "é¡¹ç›®", "é¡µé¢", "æ›´å¤š", "æŸ¥çœ‹", "è¯¦æƒ…", "ä¿¡æ¯", "åˆ—è¡¨", "å†…å®¹"}
                is_common_word = text_lower in common_words

                # å¦‚æœæœ‰ä¸­æ–‡ä¸”ä¸æ˜¯çº¯é€šç”¨è¯ï¼Œåˆ™é€šè¿‡
                if has_chinese and not is_common_word and len(text_lower) > 2:
                    logging.info(f"âœ… [UNLIMITED_MODE] æ— é™åˆ¶æ¨¡å¼é€šè¿‡: '{link_text}'")
                    logging.info(f"   åŒ¹é…URL: {raw_url}")
                    keyword_filter_pass = True
                else:
                    logging.debug(f"â­ï¸ [UNLIMITED_MODE] è·³è¿‡é€šç”¨å†…å®¹: '{link_text}'")
                    logging.debug(f"   ä¸­æ–‡: {has_chinese}, é€šç”¨è¯: {is_common_word}")
                    filtered_out += 1
                    keyword_filter_pass = False
            else:
                # æ­£å¸¸å…³é”®è¯åŒ¹é…
                if not _has_keyword(link_text, tuple(keywords) if keywords else None):
                    logging.debug(f"âŒ [CRAWLER] é»˜è®¤å…³é”®è¯è¿‡æ»¤å¤±è´¥")
                    logging.debug(f"   å¤±è´¥URL: {raw_url}")
                    filtered_out += 1
                    keyword_filter_pass = False
                else:
                    logging.info(f"âœ… [CRAWLER] é»˜è®¤å…³é”®è¯åŒ¹é…æˆåŠŸ")
                    logging.info(f"   åŒ¹é…URL: {raw_url}")
                    logging.info(f"   é“¾æ¥æ–‡æœ¬: '{link_text}'")
                    keyword_filter_pass = True

        if keyword_filter_pass:
            logging.info(f"ğŸ’¾ [CRAWLER] å…³é”®è¯åŒ¹é…é€šè¿‡ï¼Œå‡†å¤‡ä¿å­˜åˆ°æ•°æ®åº“: {raw_url}")

            try:
                # å…ˆæ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
                logging.debug(f"ğŸ” [DATABASE] æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨: {raw_url}")
                cursor.execute(
                    "SELECT COUNT(*) FROM procurement_links WHERE base_url = ? AND url = ?",
                    (base_url, raw_url)
                )
                exists = cursor.fetchone()[0] > 0
                logging.debug(f"ğŸ“‹ [DATABASE] è®°å½•å­˜åœ¨æ€§æ£€æŸ¥ç»“æœ: {exists}")

                if exists:
                    # è®°å½•å·²å­˜åœ¨ï¼Œæ‰§è¡ŒUPDATE
                    logging.debug(f"ğŸ”„ [DATABASE] æ›´æ–°ç°æœ‰è®°å½•: {raw_url}")
                    cursor.execute(
                        """
                        UPDATE procurement_links SET
                            link_text = COALESCE(?, procurement_links.link_text),
                            last_seen_at = ?,
                            is_latest = 1
                        WHERE base_url = ? AND url = ?
                        """,
                        (link_text, now, base_url, raw_url),
                    )
                    logging.info(f"âœ… [DATABASE] æˆåŠŸæ›´æ–°è®°å½•: {raw_url[:80]}")
                else:
                    # è®°å½•ä¸å­˜åœ¨ï¼Œæ‰§è¡ŒINSERT
                    logging.debug(f"â• [DATABASE] æ’å…¥æ–°è®°å½•: {raw_url}")
                    cursor.execute(
                        """
                        INSERT INTO procurement_links (
                            base_url,
                            url,
                            link_text,
                            first_seen_at,
                            last_seen_at,
                            is_latest
                        )
                        VALUES (?, ?, ?, ?, ?, 1)
                        """,
                        (base_url, raw_url, link_text, now, now),
                    )
                    logging.info(f"âœ… [DATABASE] æˆåŠŸæ–°å¢è®°å½•: {raw_url[:80]}")

                # æ— è®ºæ˜¯INSERTè¿˜æ˜¯UPDATEéƒ½ç®—ä½œæ›´æ–°
                new_or_updated += 1

            except sqlite3.Error as e:
                logging.error(f"âŒ [DATABASE] æ•°æ®åº“å†™å…¥å¤±è´¥ - URL: {raw_url}")
                logging.error(f"   é”™è¯¯è¯¦æƒ…: {e}")
                logging.debug(f"   é“¾æ¥æ–‡æœ¬: {link_text}")
                import traceback
                logging.debug(f"   å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")

    try:
        conn.commit()
        logging.info(f"ğŸ’¾ [DATABASE] æ•°æ®åº“äº‹åŠ¡æäº¤æˆåŠŸ")
    except sqlite3.Error as e:
        logging.error(f"âŒ [DATABASE] æ•°æ®åº“äº‹åŠ¡æäº¤å¤±è´¥: {e}")
        raise
    finally:
        try:
            conn.close()
            logging.debug(f"ğŸ”Œ [DATABASE] æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except:
            pass

    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    end_time = time.time()
    execution_time = end_time - start_time

    logging.info(f"ğŸ‰ [CRAWLER] çˆ¬å–ä»»åŠ¡å®Œæˆ")
    logging.info(f"â±ï¸ [CRAWLER] æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
    logging.info(f"ğŸ—„ï¸ [CRAWLER] æ•°æ®åº“è·¯å¾„: {os.path.abspath(db_path)}")
    logging.info(f"ğŸ“Š [CRAWLER] æ‰§è¡Œç»“æœç»Ÿè®¡:")
    logging.info(f"   - å‘ç°URLæ€»æ•°: {len(all_raw_urls)}")
    logging.info(f"   - å…³é”®è¯è¿‡æ»¤é€šè¿‡: {new_or_updated}")
    logging.info(f"   - å…³é”®è¯è¿‡æ»¤æ’é™¤: {filtered_out}")
    logging.info(f"   - è¿‡æ»¤é€šè¿‡ç‡: {(new_or_updated/len(all_raw_urls)*100):.1f}%" if all_raw_urls else "   - è¿‡æ»¤é€šè¿‡ç‡: 0%")

    if filtered_out > 0:
        logging.warning(f"âš ï¸ [CRAWLER] è¢«å…³é”®è¯è¿‡æ»¤æ‰çš„URLæ•°é‡: {filtered_out}")
        logging.info(f"ğŸ’¡ [CRAWLER] å»ºè®®æ£€æŸ¥å…³é”®è¯åŒ¹é…é€»è¾‘æˆ–æ‰©å±•å…³é”®è¯åˆ—è¡¨")
        if len(all_raw_urls) > 0:
            filter_rate = (filtered_out / len(all_raw_urls)) * 100
            logging.info(f"ğŸ“ˆ [CRAWLER] è¿‡æ»¤ç‡: {filter_rate:.1f}%")

    if len(all_raw_urls) == 0:
        logging.warning(f"âš ï¸ [CRAWLER] æœªå‘ç°ä»»ä½•URLï¼Œå¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜:")
        logging.warning(f"   1. ç½‘ç«™æ— æ³•è®¿é—®æˆ–åçˆ¬æœºåˆ¶")
        logging.warning(f"   2. URLè¿‡æ»¤è§„åˆ™è¿‡äºä¸¥æ ¼")
        logging.warning(f"   3. æ·±åº¦å’Œé¡µé¢å‚æ•°è®¾ç½®è¿‡å°")
        logging.warning(f"   4. ç½‘ç«™ç»“æ„ä¸åŒ…å«HTMLé¡µé¢")

    return {
        "base_url": base_url,
        "total_urls": len(all_raw_urls),
        "new_or_updated": new_or_updated,
        "filtered_out": filtered_out,
        "execution_time": execution_time,
        "db_path": db_path,
    }


async def crawl_procurement_links(
    base_url: str,
    max_depth: int | None = None,
    max_pages: int | None = None,
    keywords: list[str] | None = None,
) -> Dict[str, Any]:
    """
    Public async API used by FastAPI and the script entry point.
    åœ¨ Windows ç¯å¢ƒä¸‹ï¼ŒPlaywright çš„å¼‚æ­¥å­è¿›ç¨‹æ”¯æŒæœ‰é™ï¼Œå®¹æ˜“æŠ›å‡º NotImplementedErrorã€‚
    ä¸ºäº†ç¨³å®šæ€§ï¼ŒWindows ä¸Šç›´æ¥ä½¿ç”¨ fallbackï¼ˆrequests + BeautifulSoupï¼‰ç‰ˆæœ¬ï¼›
    å…¶å®ƒå¹³å°åˆ™ä½¿ç”¨ crawl4ai çš„ AsyncWebCrawler å®ç°æ·±åº¦çˆ¬å–ã€‚
    """
    # Windows ä¸‹ç›´æ¥èµ°å›é€€å®ç°ï¼Œå®Œå…¨ç»•è¿‡ Playwright / AsyncWebCrawler
    if sys.platform.startswith("win"):
        return await fallback_crawl_procurement_links(
            base_url, max_depth=max_depth, max_pages=max_pages, keywords=keywords
        )

    loop = asyncio.get_running_loop()

    def _worker(
        url: str,
        depth: int | None,
        pages: int | None,
        kw_list: list[str] | None,
    ) -> Dict[str, Any]:
        # Create a dedicated event loop that supports subprocess APIs.
        if sys.platform.startswith("win"):
            worker_loop = asyncio.SelectorEventLoop()
        else:
            worker_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(worker_loop)
        try:
            # Try full Playwright-based crawling first
            return worker_loop.run_until_complete(
                _crawl_procurement_links_impl(url, depth, pages, kw_list)
            )
        except NotImplementedError:
            # On Windows without proper subprocess support, fall back to requests/html parsing
            return worker_loop.run_until_complete(
                fallback_crawl_procurement_links(url, depth, pages, kw_list)
            )
        finally:
            worker_loop.close()

    return await loop.run_in_executor(None, _worker, base_url, max_depth, max_pages, keywords)


async def main() -> None:
    # Default target procurement page for standalone script usage
    base_url = "https://www.hospital-cqmu.com/gzb_cgxx"
    await crawl_procurement_links(base_url)


if __name__ == "__main__":
    asyncio.run(main())
