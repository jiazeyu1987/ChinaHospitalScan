#!/usr/bin/env python3
"""
é•¿æµ·åŒ»é™¢é“¾æ¥åˆ†æå·¥å…·
ä¸“é—¨ç”¨äºåˆ†æé•¿æµ·åŒ»é™¢ç½‘ç«™çš„é“¾æ¥ç»“æ„å’Œå†…å®¹
"""

import asyncio
import sys
import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

def analyze_longhua_links():
    """æ·±å…¥åˆ†æé•¿æµ·åŒ»é™¢ç½‘ç«™çš„é“¾æ¥"""
    base_url = "https://www.longhua.net/index/cggg.htm"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    print("ğŸ” å¼€å§‹åˆ†æé•¿æµ·åŒ»é™¢é“¾æ¥ç»“æ„")
    print(f"ç›®æ ‡URL: {base_url}")
    print("="*80)

    try:
        # è¯·æ±‚é¡µé¢
        print("ğŸ“¥ æ­£åœ¨è·å–é¡µé¢å†…å®¹...")
        response = requests.get(base_url, headers=headers, timeout=30)
        response.raise_for_status()

        print(f"âœ… é¡µé¢è¯·æ±‚æˆåŠŸ")
        print(f"   çŠ¶æ€ç : {response.status_code}")
        print(f"   å†…å®¹é•¿åº¦: {len(response.content)} bytes")
        print(f"   Content-Type: {response.headers.get('content-type', 'unknown')}")

        # è§£æHTML
        soup = BeautifulSoup(response.content, "html.parser")

        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all("a", href=True)
        print(f"\nğŸ”— å‘ç°æ€»é“¾æ¥æ•°: {len(all_links)}")

        # åˆ†æé“¾æ¥
        domain_links = []
        external_links = []
        js_links = []
        other_links = []

        html_extensions = ['.html', '.htm', '.shtml']

        for i, link in enumerate(all_links):
            href = link.get("href")
            text = link.get_text(strip=True)

            if not href:
                other_links.append((i+1, href, text, "ç©ºé“¾æ¥"))
                continue

            # URLåˆ†ç±»
            if href.startswith("javascript:") or href.startswith("#"):
                js_links.append((i+1, href, text, "JSé“¾æ¥"))
            elif href.startswith("http"):
                if "longhua.net" in href:
                    domain_links.append((i+1, href, text, "åŒåŸŸé“¾æ¥"))
                else:
                    external_links.append((i+1, href, text, "å¤–åŸŸé“¾æ¥"))
            else:
                domain_links.append((i+1, href, text, "ç›¸å¯¹é“¾æ¥"))

        print(f"\nğŸ“Š é“¾æ¥åˆ†ç±»ç»Ÿè®¡:")
        print(f"   åŒåŸŸé“¾æ¥: {len(domain_links)}")
        print(f"   å¤–åŸŸé“¾æ¥: {len(external_links)}")
        print(f"   JSé“¾æ¥: {len(js_links)}")
        print(f"   å…¶ä»–é“¾æ¥: {len(other_links)}")

        # è¯¦ç»†åˆ†æåŒåŸŸé“¾æ¥
        print(f"\nğŸ¯ åŒåŸŸé“¾æ¥è¯¦ç»†åˆ†æ (å‰20ä¸ª):")
        print("-"*80)

        html_count = 0
        html_links = []

        for i, (num, href, text, category) in enumerate(domain_links[:20]):
            # è½¬æ¢ä¸ºç»å¯¹URL
            if href.startswith("/"):
                absolute_href = f"https://www.longhua.net{href}"
            elif not href.startswith("http"):
                absolute_href = f"https://www.longhua.net/{href.lstrip('/')}"
            else:
                absolute_href = href

            # æ£€æŸ¥æ˜¯å¦ä¸ºHTMLé¡µé¢
            parsed = urlparse(absolute_href)
            path = parsed.path or ""
            is_html = any(path.lower().endswith(ext) for ext in html_extensions) or not any(path.lower().endswith(ext) for ext in
                ['.css', '.js', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.ico', '.pdf',
                 '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.tar', '.gz',
                 '.mp3', '.mp4', '.avi', '.mov', '.flv', '.wmv'])

            if is_html:
                html_count += 1
                html_links.append((num, href, text, category))

            # å…³é”®è¯æ£€æŸ¥
            keywords = ["å…¬å‘Š", "é‡‡è´­", "æ‹›æ ‡", "è®¾å¤‡", "åŒ»ç–—", "å™¨æ¢°", "ä¸­æ ‡", "ç»“æœ"]
            matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]

            print(f"{num:2d}. {category}")
            print(f"     URL: {href}")
            print(f"     æ–‡æœ¬: {text[:100]}..." if len(text) > 100 else f"     æ–‡æœ¬: {text}")
            print(f"     HTMLé¡µé¢: {'âœ…' if is_html else 'âŒ'}")
            print(f"     å…³é”®è¯åŒ¹é…: {matched_keywords if matched_keywords else 'âŒ æ— åŒ¹é…'}")
            print()

        print(f"\nğŸ“ˆ åŒåŸŸHTMLé“¾æ¥ç»Ÿè®¡:")
        print(f"   HTMLé“¾æ¥æ•°: {html_count}")
        print(f"   éHTMLé“¾æ¥æ•°: {len(domain_links) - html_count}")

        # è¾“å‡ºæ‰€æœ‰HTMLé“¾æ¥ç”¨äºè¿›ä¸€æ­¥åˆ†æ
        print(f"\nğŸ¯ æ‰€æœ‰åŒåŸŸHTMLé“¾æ¥åˆ—è¡¨:")
        print("-"*80)
        for num, href, text, category in html_links:
            matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]
            print(f"{num:2d}. {href} -> '{text}' (å…³é”®è¯: {matched_keywords})")

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_longhua_links()